# LSTM Hochreiter and Schmidhuber '97 and Gers and Schmidhuber '01

We're starting this review of syntax-y machine learning papers with [*the* LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf "Hochreiter and Schmidhuber '97") and jumping from that to [Gers and Schmidhuber's](https://ieeexplore.ieee.org/document/963769 "IEEE 2001 Paper") inquisition of what formal language types LSTM's can learn. On to business, the introduction to LSTM's...

## 1997 and the beginning of the LSTM machine

### Abstract
Back propagation through RNNs can have a hard time learning long distance relationships. LSTM's can address this by providing a shortcut for error to flow through to long distance history representations in a way to previous architecture could. Important jargon for this includes: constant error carrousels (CECs), gate units. 

### Intro













### A Post Script for things I'd like to learn eventually
- How does attention's attempted replacement of RNN's relate to LSTMs?
- Do word embeddings learn TAM and PHI Features (no it's not related, but I still want to remember it)
