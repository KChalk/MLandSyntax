# Summarizing _Representation of Linguistic Form and Function in Neural Networks_ by Ákos Kádár, Grzegorz Chrupała and Afra Alishahi

This paper attempts to identify which parts of a sentence are most important for language modeling and how this differs across two tasks. To do so, they measure distance between trained-model-generated representations of A) a complete sentence and B) the same sentence with a single word ommitted. The paper analyzes how these 'omission scores' change depending on which word was omitted, the syntactic category of the omitted word, and the omitted word's dependency labels. They compare these across models trained on two different tasks: one a traditional lanugage model predicting the next work in the sentence, and one trained to match captions to images. The imaginet model introduced by the same authors in 2015 contains both of these models with a shared word embedding space and is the primary architecture used here. 

The authors interpret their results to show that the image related task has higher ommision scores for and thus gleans more information from semantic words (nouns and adjectives), while the purely language based model pays more even attention to the rest of the sentence, including functional words. In fact, linear regression suggests that the best predictor of a word's omission score in the language model is its position in the sentence. This seems to be an artifact of RNN sentence representations, in which signals are known to be stronger for words which occur at the end of the sentence. The same regression shows that the image model pays the most attention to the second word in the sentence, which seems intuitive given that is the most likely position for a topic in an English image caption. 

Over all, the introduction of the omission score and its use in this context are the main contributions of this paper. Note that two separate versions of the paper were published. This summary is based on that in Computational Linguistics 2017: https://doi.org/10.1162/COLI_a_00300. A 2016 version with different figures is available as arXiv:1602.08952. 